{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4a83fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55, 531, 25, 241, 12, 129, 394, 44, 492, 3326, 15068, 58, 148, 56, 43, 8, 1004, 6, 474, 48, 30, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 727, 1715, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 45, 301, 782, 3624, 14627, 15, 12612, 277, 5, 216, 56, 36, 2119, 3, 9, 19529, 593, 853, 21, 921, 113, 2746, 12, 129, 394, 28, 70, 17712, 1098, 5, 216, 56, 3884, 25, 762, 25, 174, 12, 214, 12, 5978, 16, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 379, 2097, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 11, 27856, 6, 303, 24190, 11, 1472, 251, 5, 37, 583, 12, 36, 16, 8, 853, 19, 25264, 399, 568, 6, 11, 21, 21380, 7, 34, 19, 339, 5, 15746, 26, 16, 8, 583, 56, 36, 893, 3, 9, 3, 17, 18, 9486, 42, 3, 9, 1409, 29, 11, 25, 56, 36, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the T5 tokenizer.\n",
    "# It's recommended to use a tokenizer from a pre-trained model like 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# 2. Load the C4 dataset.\n",
    "# The `streaming=True` argument is useful for huge datasets like C4 to avoid downloading the whole thing.\n",
    "c4_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "\n",
    "# 3. Define a tokenization function.\n",
    "# This function will be applied to each batch of data.\n",
    "def tokenize_function(examples):\n",
    "    # The T5 model expects a prefix for the task, for example \"denoise text: \".\n",
    "    # This is important for T5's pre-training objective.\n",
    "    # However, for a simple tokenization, we can just process the \"text\" field.\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# 4. Apply the tokenizer to the dataset using the map function.\n",
    "# `batched=True` processes the data in batches, which is much faster.\n",
    "tokenized_c4 = c4_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# You can now iterate through the tokenized dataset.\n",
    "for example in tokenized_c4:\n",
    "    print(example[\"input_ids\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d17fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 37\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4158/3157935005.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/7], Loss: 3.5055\n",
      "Epoch [1/20], Step [2/7], Loss: 3.5048\n",
      "Epoch [1/20], Step [3/7], Loss: 3.5556\n",
      "Epoch [1/20], Step [4/7], Loss: 3.5737\n",
      "Epoch [1/20], Step [5/7], Loss: 3.6062\n",
      "Epoch [1/20], Step [6/7], Loss: 3.6068\n",
      "Epoch [1/20], Step [7/7], Loss: 3.5356\n",
      "Epoch [2/20], Step [1/7], Loss: 3.0164\n",
      "Epoch [2/20], Step [2/7], Loss: 3.3542\n",
      "Epoch [2/20], Step [3/7], Loss: 3.2354\n",
      "Epoch [2/20], Step [4/7], Loss: 3.3654\n",
      "Epoch [2/20], Step [5/7], Loss: 3.4057\n",
      "Epoch [2/20], Step [6/7], Loss: 3.2739\n",
      "Epoch [2/20], Step [7/7], Loss: 3.2412\n",
      "Epoch [3/20], Step [1/7], Loss: 3.0110\n",
      "Epoch [3/20], Step [2/7], Loss: 3.0959\n",
      "Epoch [3/20], Step [3/7], Loss: 2.7066\n",
      "Epoch [3/20], Step [4/7], Loss: 2.7557\n",
      "Epoch [3/20], Step [5/7], Loss: 3.0402\n",
      "Epoch [3/20], Step [6/7], Loss: 2.8373\n",
      "Epoch [3/20], Step [7/7], Loss: 2.7417\n",
      "Epoch [4/20], Step [1/7], Loss: 2.2442\n",
      "Epoch [4/20], Step [2/7], Loss: 2.3088\n",
      "Epoch [4/20], Step [3/7], Loss: 2.1260\n",
      "Epoch [4/20], Step [4/7], Loss: 2.0433\n",
      "Epoch [4/20], Step [5/7], Loss: 2.1352\n",
      "Epoch [4/20], Step [6/7], Loss: 2.0577\n",
      "Epoch [4/20], Step [7/7], Loss: 2.2903\n",
      "Epoch [5/20], Step [1/7], Loss: 2.6069\n",
      "Epoch [5/20], Step [2/7], Loss: 2.1634\n",
      "Epoch [5/20], Step [3/7], Loss: 2.1639\n",
      "Epoch [5/20], Step [4/7], Loss: 2.3418\n",
      "Epoch [5/20], Step [5/7], Loss: 2.2676\n",
      "Epoch [5/20], Step [6/7], Loss: 2.2052\n",
      "Epoch [5/20], Step [7/7], Loss: 2.3103\n",
      "Epoch [6/20], Step [1/7], Loss: 2.5423\n",
      "Epoch [6/20], Step [2/7], Loss: 2.0880\n",
      "Epoch [6/20], Step [3/7], Loss: 1.5514\n",
      "Epoch [6/20], Step [4/7], Loss: 1.3167\n",
      "Epoch [6/20], Step [5/7], Loss: 1.2604\n",
      "Epoch [6/20], Step [6/7], Loss: 1.2412\n",
      "Epoch [6/20], Step [7/7], Loss: 1.3669\n",
      "Epoch [7/20], Step [1/7], Loss: 0.7167\n",
      "Epoch [7/20], Step [2/7], Loss: 0.6875\n",
      "Epoch [7/20], Step [3/7], Loss: 0.7069\n",
      "Epoch [7/20], Step [4/7], Loss: 1.0147\n",
      "Epoch [7/20], Step [5/7], Loss: 1.2258\n",
      "Epoch [7/20], Step [6/7], Loss: 1.2288\n",
      "Epoch [7/20], Step [7/7], Loss: 1.1816\n",
      "Epoch [8/20], Step [1/7], Loss: 0.8942\n",
      "Epoch [8/20], Step [2/7], Loss: 1.0039\n",
      "Epoch [8/20], Step [3/7], Loss: 1.0273\n",
      "Epoch [8/20], Step [4/7], Loss: 0.8541\n",
      "Epoch [8/20], Step [5/7], Loss: 1.0853\n",
      "Epoch [8/20], Step [6/7], Loss: 1.0561\n",
      "Epoch [8/20], Step [7/7], Loss: 0.9722\n",
      "Epoch [9/20], Step [1/7], Loss: 1.2588\n",
      "Epoch [9/20], Step [2/7], Loss: 0.6594\n",
      "Epoch [9/20], Step [3/7], Loss: 0.6251\n",
      "Epoch [9/20], Step [4/7], Loss: 0.6464\n",
      "Epoch [9/20], Step [5/7], Loss: 0.8248\n",
      "Epoch [9/20], Step [6/7], Loss: 0.6985\n",
      "Epoch [9/20], Step [7/7], Loss: 0.5999\n",
      "Epoch [10/20], Step [1/7], Loss: 0.1191\n",
      "Epoch [10/20], Step [2/7], Loss: 0.0995\n",
      "Epoch [10/20], Step [3/7], Loss: 0.0741\n",
      "Epoch [10/20], Step [4/7], Loss: 0.1306\n",
      "Epoch [10/20], Step [5/7], Loss: 0.1059\n",
      "Epoch [10/20], Step [6/7], Loss: 0.1672\n",
      "Epoch [10/20], Step [7/7], Loss: 0.1441\n",
      "Epoch [11/20], Step [1/7], Loss: 0.0057\n",
      "Epoch [11/20], Step [2/7], Loss: 0.1181\n",
      "Epoch [11/20], Step [3/7], Loss: 0.1183\n",
      "Epoch [11/20], Step [4/7], Loss: 0.1216\n",
      "Epoch [11/20], Step [5/7], Loss: 0.2888\n",
      "Epoch [11/20], Step [6/7], Loss: 0.2531\n",
      "Epoch [11/20], Step [7/7], Loss: 0.2287\n",
      "Epoch [12/20], Step [1/7], Loss: 0.0251\n",
      "Epoch [12/20], Step [2/7], Loss: 0.0478\n",
      "Epoch [12/20], Step [3/7], Loss: 0.0698\n",
      "Epoch [12/20], Step [4/7], Loss: 0.0747\n",
      "Epoch [12/20], Step [5/7], Loss: 0.0603\n",
      "Epoch [12/20], Step [6/7], Loss: 0.0702\n",
      "Epoch [12/20], Step [7/7], Loss: 0.0894\n",
      "Epoch [13/20], Step [1/7], Loss: 0.0180\n",
      "Epoch [13/20], Step [2/7], Loss: 0.0949\n",
      "Epoch [13/20], Step [3/7], Loss: 0.1285\n",
      "Epoch [13/20], Step [4/7], Loss: 0.1055\n",
      "Epoch [13/20], Step [5/7], Loss: 0.1251\n",
      "Epoch [13/20], Step [6/7], Loss: 0.1476\n",
      "Epoch [13/20], Step [7/7], Loss: 0.1274\n",
      "Epoch [14/20], Step [1/7], Loss: 0.0191\n",
      "Epoch [14/20], Step [2/7], Loss: 0.0109\n",
      "Epoch [14/20], Step [3/7], Loss: 0.0179\n",
      "Epoch [14/20], Step [4/7], Loss: 0.2913\n",
      "Epoch [14/20], Step [5/7], Loss: 0.4005\n",
      "Epoch [14/20], Step [6/7], Loss: 0.3373\n",
      "Epoch [14/20], Step [7/7], Loss: 0.4240\n",
      "Epoch [15/20], Step [1/7], Loss: 0.0543\n",
      "Epoch [15/20], Step [2/7], Loss: 0.0400\n",
      "Epoch [15/20], Step [3/7], Loss: 0.0388\n",
      "Epoch [15/20], Step [4/7], Loss: 0.0313\n",
      "Epoch [15/20], Step [5/7], Loss: 0.1307\n",
      "Epoch [15/20], Step [6/7], Loss: 0.1166\n",
      "Epoch [15/20], Step [7/7], Loss: 0.1000\n",
      "Epoch [16/20], Step [1/7], Loss: 0.0132\n",
      "Epoch [16/20], Step [2/7], Loss: 0.0068\n",
      "Epoch [16/20], Step [3/7], Loss: 0.0602\n",
      "Epoch [16/20], Step [4/7], Loss: 0.0758\n",
      "Epoch [16/20], Step [5/7], Loss: 0.0607\n",
      "Epoch [16/20], Step [6/7], Loss: 0.0603\n",
      "Epoch [16/20], Step [7/7], Loss: 0.0532\n",
      "Epoch [17/20], Step [1/7], Loss: 0.9317\n",
      "Epoch [17/20], Step [2/7], Loss: 0.4883\n",
      "Epoch [17/20], Step [3/7], Loss: 0.3262\n",
      "Epoch [17/20], Step [4/7], Loss: 0.3658\n",
      "Epoch [17/20], Step [5/7], Loss: 0.2946\n",
      "Epoch [17/20], Step [6/7], Loss: 0.2531\n",
      "Epoch [17/20], Step [7/7], Loss: 0.2689\n",
      "Epoch [18/20], Step [1/7], Loss: 0.0638\n",
      "Epoch [18/20], Step [2/7], Loss: 0.0336\n",
      "Epoch [18/20], Step [3/7], Loss: 0.0261\n",
      "Epoch [18/20], Step [4/7], Loss: 0.0230\n",
      "Epoch [18/20], Step [5/7], Loss: 0.0196\n",
      "Epoch [18/20], Step [6/7], Loss: 0.0419\n",
      "Epoch [18/20], Step [7/7], Loss: 0.0382\n",
      "Epoch [19/20], Step [1/7], Loss: 0.0008\n",
      "Epoch [19/20], Step [2/7], Loss: 0.0574\n",
      "Epoch [19/20], Step [3/7], Loss: 0.0409\n",
      "Epoch [19/20], Step [4/7], Loss: 0.0644\n",
      "Epoch [19/20], Step [5/7], Loss: 0.1218\n",
      "Epoch [19/20], Step [6/7], Loss: 0.1202\n",
      "Epoch [19/20], Step [7/7], Loss: 0.1030\n",
      "Epoch [20/20], Step [1/7], Loss: 0.0025\n",
      "Epoch [20/20], Step [2/7], Loss: 0.0027\n",
      "Epoch [20/20], Step [3/7], Loss: 0.0202\n",
      "Epoch [20/20], Step [4/7], Loss: 0.0152\n",
      "Epoch [20/20], Step [5/7], Loss: 0.0150\n",
      "Epoch [20/20], Step [6/7], Loss: 0.1024\n",
      "Epoch [20/20], Step [7/7], Loss: 0.0878\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Define a simple text dataset and vocabulary ---\n",
    "# FIX: Added much longer sentences to the training data.\n",
    "TEXT_DATA = [\n",
    "    \"hello world and this is a much longer sentence than before\",\n",
    "    \"python programming is fun and easy to learn especially for beginners\",\n",
    "    \"i love deep learning with convolutional neural networks and attention\",\n",
    "    \"deep learning is a powerful tool in artificial intelligence for language modeling tasks\"\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = sorted(list(set(\" \".join(TEXT_DATA).split())))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, word_to_idx, sequence_length):\n",
    "        self.sequences = []\n",
    "        for sentence in text_data:\n",
    "            words = sentence.split()\n",
    "            # This condition will now be met for the longer sentences\n",
    "            if len(words) > sequence_length:\n",
    "                for i in range(len(words) - sequence_length):\n",
    "                    input_seq = [word_to_idx[word] for word in words[i:i+sequence_length]]\n",
    "                    target_word = word_to_idx[words[i+sequence_length]]\n",
    "                    self.sequences.append((torch.tensor(input_seq), torch.tensor(target_word)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# --- 2. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 3. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * self.final_seq_len, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 2\n",
    "    num_epochs = 20\n",
    "    # FIX: Increased sequence length to a value that won't result in an empty tensor.\n",
    "    sequence_length = 8\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Data loading\n",
    "    train_dataset = TextDataset(TEXT_DATA, word_to_idx, sequence_length)\n",
    "    \n",
    "    # FIX: Added a check to prevent the error\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "        print(f\"Please increase the length of sentences in TEXT_DATA or decrease the `sequence_length` (currently {sequence_length}).\")\n",
    "        return\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e318639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading a small streaming portion of the C4 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4158/1642505007.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/25], Step [50], Loss: 8.2158\n",
      "Epoch [1/25], Step [100], Loss: 7.9000\n",
      "Epoch [1/25], Step [150], Loss: 7.9383\n",
      "Epoch [1/25], Step [200], Loss: 7.5997\n",
      "Epoch [1/25], Step [250], Loss: 7.5093\n",
      "Epoch [1/25], Step [300], Loss: 7.4525\n",
      "Epoch [1/25], Step [350], Loss: 7.6379\n",
      "Epoch [1/25], Step [400], Loss: 7.7360\n",
      "Epoch [1/25], Step [450], Loss: 7.6294\n",
      "Epoch [1/25], Step [500], Loss: 7.5267\n",
      "Epoch [1/25], Step [550], Loss: 7.8110\n",
      "Epoch [1/25], Step [600], Loss: 7.6373\n",
      "Epoch [1/25], Step [650], Loss: 7.5378\n",
      "Epoch [1/25], Step [700], Loss: 7.4987\n",
      "Epoch [1/25], Step [750], Loss: 7.4935\n",
      "Epoch [1/25], Step [800], Loss: 8.0246\n",
      "Epoch [1/25], Step [850], Loss: 7.7029\n",
      "Epoch [1/25], Step [900], Loss: 7.6623\n",
      "Epoch [1/25], Step [950], Loss: 7.6113\n",
      "Epoch [1/25], Step [1000], Loss: 7.7125\n",
      "Epoch [1/25], Step [1050], Loss: 7.4421\n",
      "Epoch [1/25], Step [1100], Loss: 7.3279\n",
      "Epoch [1/25], Step [1150], Loss: 7.6664\n",
      "Epoch [1/25], Step [1200], Loss: 7.5105\n",
      "Epoch [1/25], Step [1250], Loss: 7.1405\n",
      "Epoch [1/25], Step [1300], Loss: 7.6795\n",
      "Epoch [1/25], Step [1350], Loss: 7.1877\n",
      "Epoch [1/25], Step [1400], Loss: 7.1598\n",
      "Epoch [1/25], Step [1450], Loss: 7.5839\n",
      "Epoch [1/25], Step [1500], Loss: 7.1749\n",
      "Epoch [1/25], Step [1550], Loss: 7.9842\n",
      "Epoch [1/25], Step [1600], Loss: 7.5168\n",
      "Epoch [1/25], Step [1650], Loss: 6.9982\n",
      "Epoch [1/25], Step [1700], Loss: 7.7029\n",
      "Epoch [1/25], Step [1750], Loss: 7.8843\n",
      "Epoch [1/25], Step [1800], Loss: 7.1038\n",
      "Epoch [1/25], Step [1850], Loss: 7.6622\n",
      "Epoch [1/25], Step [1900], Loss: 7.2577\n",
      "Epoch [1/25], Step [1950], Loss: 7.4813\n",
      "Epoch [1/25], Step [2000], Loss: 7.8571\n",
      "Epoch [1/25], Step [2050], Loss: 7.1885\n",
      "Epoch [1/25], Step [2100], Loss: 7.2365\n",
      "Epoch [1/25], Step [2150], Loss: 7.5826\n",
      "Epoch [1/25], Step [2200], Loss: 7.8168\n",
      "Epoch [1/25], Step [2250], Loss: 7.7404\n",
      "Epoch [1/25], Step [2300], Loss: 7.7256\n",
      "Epoch [1/25], Step [2350], Loss: 7.1054\n",
      "Epoch [1/25], Step [2400], Loss: 7.5373\n",
      "Epoch [1/25], Step [2450], Loss: 7.3354\n",
      "Epoch [1/25], Step [2500], Loss: 7.3790\n",
      "Epoch [1/25], Step [2550], Loss: 7.1707\n",
      "Epoch [1/25], Step [2600], Loss: 7.3981\n",
      "Epoch [1/25], Step [2650], Loss: 7.2334\n",
      "Epoch [1/25], Step [2700], Loss: 7.2367\n",
      "Epoch [1/25], Step [2750], Loss: 7.0971\n",
      "Epoch [1/25], Step [2800], Loss: 7.4220\n",
      "Epoch [1/25], Step [2850], Loss: 7.4576\n",
      "Epoch [1/25], Step [2900], Loss: 7.5420\n",
      "Epoch [1/25], Step [2950], Loss: 7.5452\n",
      "Epoch [1/25], Step [3000], Loss: 7.6435\n",
      "Epoch [1/25], Step [3050], Loss: 7.5033\n",
      "Epoch [1/25], Step [3100], Loss: 7.2311\n",
      "Epoch [1/25], Step [3150], Loss: 7.3044\n",
      "Epoch [1/25], Step [3200], Loss: 6.9491\n",
      "Epoch [1/25], Step [3250], Loss: 7.0954\n",
      "Epoch [1/25], Step [3300], Loss: 7.8953\n",
      "Epoch [1/25], Step [3350], Loss: 6.9175\n",
      "Epoch [1/25], Step [3400], Loss: 7.4096\n",
      "Epoch [1/25], Step [3450], Loss: 7.2929\n",
      "Epoch [1/25], Step [3500], Loss: 7.2159\n",
      "Epoch [1/25], Step [3550], Loss: 7.1474\n",
      "Epoch [1/25], Step [3600], Loss: 7.3395\n",
      "Epoch [1/25], Step [3650], Loss: 7.2814\n",
      "Epoch [1/25], Step [3700], Loss: 7.1174\n",
      "Epoch [1/25], Step [3750], Loss: 7.4193\n",
      "Epoch [1/25], Step [3800], Loss: 7.1122\n",
      "Epoch [1/25], Step [3850], Loss: 6.9207\n",
      "Epoch [1/25], Step [3900], Loss: 7.7709\n",
      "Epoch [1/25], Step [3950], Loss: 7.4465\n",
      "Epoch [1/25], Step [4000], Loss: 7.2859\n",
      "Epoch [1/25], Step [4050], Loss: 7.2400\n",
      "Epoch [1/25], Step [4100], Loss: 6.9594\n",
      "Epoch [1/25], Step [4150], Loss: 7.0810\n",
      "Epoch [1/25], Step [4200], Loss: 7.5904\n",
      "Epoch [1/25], Step [4250], Loss: 7.3283\n",
      "Epoch [1/25], Step [4300], Loss: 7.4464\n",
      "Epoch [1/25], Step [4350], Loss: 7.5004\n",
      "Epoch [1/25], Step [4400], Loss: 7.0464\n",
      "Epoch [1/25], Step [4450], Loss: 7.0380\n",
      "Epoch [1/25], Step [4500], Loss: 6.9685\n",
      "Epoch [1/25], Step [4550], Loss: 7.4460\n",
      "Epoch [1/25], Step [4600], Loss: 7.5286\n",
      "Epoch [1/25], Step [4650], Loss: 7.6041\n",
      "Epoch [1/25], Step [4700], Loss: 7.7325\n",
      "Epoch [1/25], Step [4750], Loss: 7.1799\n",
      "Epoch [1/25], Step [4800], Loss: 7.2137\n",
      "Epoch [1/25], Step [4850], Loss: 7.2940\n",
      "Epoch [1/25], Step [4900], Loss: 7.3346\n",
      "Epoch [1/25], Step [4950], Loss: 7.2454\n",
      "Epoch [1/25], Step [5000], Loss: 7.0170\n",
      "Epoch [1/25], Step [5050], Loss: 7.0668\n",
      "Epoch [1/25], Step [5100], Loss: 7.6593\n",
      "Epoch [1/25], Step [5150], Loss: 6.8808\n",
      "Epoch [1/25], Step [5200], Loss: 7.8151\n",
      "Epoch [1/25], Step [5250], Loss: 7.1153\n",
      "Epoch [1/25], Step [5300], Loss: 7.1210\n",
      "Epoch [1/25], Step [5350], Loss: 7.7180\n",
      "Epoch [1/25], Step [5400], Loss: 7.3621\n",
      "Epoch [1/25], Step [5450], Loss: 7.5550\n",
      "Epoch [1/25], Step [5500], Loss: 7.6800\n",
      "Epoch [1/25], Step [5550], Loss: 7.1184\n",
      "Epoch [1/25], Step [5600], Loss: 7.2174\n",
      "Epoch [1/25], Step [5650], Loss: 7.4201\n",
      "Epoch [1/25], Step [5700], Loss: 7.2794\n",
      "Epoch [1/25], Step [5750], Loss: 7.3428\n",
      "Epoch [1/25], Step [5800], Loss: 7.2689\n",
      "Epoch [1/25], Step [5850], Loss: 7.3998\n",
      "Epoch [1/25], Step [5900], Loss: 7.5089\n",
      "Epoch [1/25], Step [5950], Loss: 7.4570\n",
      "Epoch [1/25], Step [6000], Loss: 7.0251\n",
      "Epoch [1/25], Step [6050], Loss: 7.1341\n",
      "Epoch [1/25], Step [6100], Loss: 7.3183\n",
      "Epoch [1/25], Step [6150], Loss: 7.4981\n",
      "Epoch [1/25], Step [6200], Loss: 7.4884\n",
      "Epoch [1/25], Step [6250], Loss: 7.5420\n",
      "Epoch [1/25], Step [6300], Loss: 7.3688\n",
      "Epoch [1/25], Step [6350], Loss: 7.1549\n",
      "Epoch [1/25], Step [6400], Loss: 7.2900\n",
      "Epoch [1/25], Step [6450], Loss: 7.2140\n",
      "Epoch [1/25], Step [6500], Loss: 6.9567\n",
      "Epoch [1/25], Step [6550], Loss: 7.0468\n",
      "Epoch [1/25], Step [6600], Loss: 7.2514\n",
      "Epoch [1/25], Step [6650], Loss: 7.2824\n",
      "Epoch [1/25], Step [6700], Loss: 7.2739\n",
      "Epoch [1/25], Step [6750], Loss: 7.3067\n",
      "Epoch [1/25], Step [6800], Loss: 7.5236\n",
      "Epoch [1/25], Step [6850], Loss: 6.8935\n",
      "Epoch [1/25], Step [6900], Loss: 7.1295\n",
      "Epoch [1/25], Step [6950], Loss: 7.6610\n",
      "Epoch [1/25], Step [7000], Loss: 7.0590\n",
      "Epoch [1/25], Step [7050], Loss: 6.9301\n",
      "Epoch [1/25], Step [7100], Loss: 6.9745\n",
      "Epoch [1/25], Step [7150], Loss: 7.1656\n",
      "Epoch [1/25], Step [7200], Loss: 7.2016\n",
      "Epoch [1/25], Step [7250], Loss: 7.0580\n",
      "Epoch [1/25], Step [7300], Loss: 7.0411\n",
      "Epoch [1/25], Step [7350], Loss: 7.5956\n",
      "Epoch [1/25], Step [7400], Loss: 7.3010\n",
      "Epoch [1/25], Step [7450], Loss: 6.8878\n",
      "Epoch [1/25], Step [7500], Loss: 7.1882\n",
      "Epoch [1/25], Step [7550], Loss: 7.5895\n",
      "Epoch [1/25], Step [7600], Loss: 6.9706\n",
      "Epoch [1/25], Step [7650], Loss: 7.2061\n",
      "Epoch [1/25], Step [7700], Loss: 7.3480\n",
      "Epoch [1/25], Step [7750], Loss: 6.8599\n",
      "Epoch [1/25], Step [7800], Loss: 7.0448\n",
      "Epoch [1/25], Step [7850], Loss: 7.2222\n",
      "Epoch [1/25], Step [7900], Loss: 7.3010\n",
      "Epoch [1/25], Step [7950], Loss: 7.7617\n",
      "Epoch [1/25], Step [8000], Loss: 7.2604\n",
      "Epoch [1/25], Step [8050], Loss: 7.1309\n",
      "Epoch [1/25], Step [8100], Loss: 7.4881\n",
      "Epoch [1/25], Step [8150], Loss: 7.2453\n",
      "Epoch [1/25], Step [8200], Loss: 7.1648\n",
      "Epoch [1/25], Step [8250], Loss: 7.4221\n",
      "Epoch [1/25], Step [8300], Loss: 7.2271\n",
      "Epoch [1/25], Step [8350], Loss: 6.9883\n",
      "Epoch [1/25], Step [8400], Loss: 7.3023\n",
      "Epoch [1/25], Step [8450], Loss: 6.9374\n",
      "Epoch [1/25], Step [8500], Loss: 7.5330\n",
      "Epoch [1/25], Step [8550], Loss: 7.2297\n",
      "Epoch [1/25], Step [8600], Loss: 7.3874\n",
      "Epoch [1/25], Step [8650], Loss: 7.4583\n",
      "Epoch [1/25], Step [8700], Loss: 7.1164\n",
      "Epoch [1/25], Step [8750], Loss: 7.2337\n",
      "Epoch [1/25], Step [8800], Loss: 7.1067\n",
      "Epoch [1/25], Step [8850], Loss: 7.3341\n",
      "Epoch [1/25], Step [8900], Loss: 6.9518\n",
      "Epoch [1/25], Step [8950], Loss: 7.6097\n",
      "Epoch [1/25], Step [9000], Loss: 7.0986\n",
      "Epoch [1/25], Step [9050], Loss: 7.9211\n",
      "Epoch [1/25], Step [9100], Loss: 6.7237\n",
      "Epoch [1/25], Step [9150], Loss: 7.2044\n",
      "Epoch [1/25], Step [9200], Loss: 7.0457\n",
      "Epoch [1/25], Step [9250], Loss: 6.9969\n",
      "Epoch [1/25], Step [9300], Loss: 6.9480\n",
      "Epoch [1/25], Step [9350], Loss: 7.3407\n",
      "Epoch [1/25], Step [9400], Loss: 7.3114\n",
      "Epoch [1/25], Step [9450], Loss: 7.1905\n",
      "Epoch [1/25], Step [9500], Loss: 7.2788\n",
      "Epoch [1/25], Step [9550], Loss: 7.0343\n",
      "Epoch [1/25], Step [9600], Loss: 7.1850\n",
      "Epoch [1/25], Step [9650], Loss: 7.0339\n",
      "Epoch [1/25], Step [9700], Loss: 7.1640\n",
      "Epoch [1/25], Step [9750], Loss: 7.1013\n",
      "Epoch [1/25], Step [9800], Loss: 7.3969\n",
      "Epoch [1/25], Step [9850], Loss: 7.0492\n",
      "Epoch [1/25], Step [9900], Loss: 7.1536\n",
      "Epoch [1/25], Step [9950], Loss: 7.1300\n",
      "Epoch [1/25], Step [10000], Loss: 7.0992\n",
      "Epoch [1/25], Step [10050], Loss: 7.2688\n",
      "Epoch [1/25], Step [10100], Loss: 6.9949\n",
      "Epoch [1/25], Step [10150], Loss: 7.2039\n",
      "Epoch [1/25], Step [10200], Loss: 7.4737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining finished.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     train_model()\n",
      "Cell \u001b[0;32mIn[5], line 188\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    186\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39m'\u001b[39;49m\u001b[39mlanguage-model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    190\u001b[0m \u001b[39m# FIX: The total number of batches is unknown, so average loss is not meaningful.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Total Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;49;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;49;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        # We need a causality mask for language modeling\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 2. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # Conv1d expects input shape (batch, channels, sequence_length)\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        # FIX: Removed nn.Flatten() and adjusted Linear layer to work on a per-token basis.\n",
    "        # This is the correct way to build a sequence-to-sequence model for language modeling.\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        # FIX: The fc_block now processes the attended features without flattening.\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "# --- 3. Main training function updated for C4 dataset ---\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 8\n",
    "    num_epochs = 25\n",
    "    sequence_length = 128\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 1. Load C4 dataset (streaming) and Bert tokenizer\n",
    "    # ----------------------------------------\n",
    "    print(\"Loading a small streaming portion of the C4 dataset...\")\n",
    "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Use a simpler, pre-trained tokenizer: BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    # We will use a smaller sample for a quicker demonstration.\n",
    "    dataset_sample = dataset\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. Pre-process the dataset with the tokenizer\n",
    "    # ----------------------------------------\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"text\"]\n",
    "        tokenized_input = tokenizer(\n",
    "            inputs,\n",
    "            max_length=sequence_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Shift the labels for next-token prediction\n",
    "        input_ids = tokenized_input['input_ids'].squeeze(0)\n",
    "        labels = torch.cat((input_ids[1:], torch.tensor([tokenizer.pad_token_id])))\n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "    \n",
    "    # Apply the preprocessing to the streaming dataset\n",
    "    processed_dataset = dataset_sample.map(preprocess_function, batched=False)\n",
    "    \n",
    "    # We use the processed streaming dataset directly with the DataLoader\n",
    "    train_loader = DataLoader(processed_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 3. Model, Loss, and Optimizer\n",
    "    # ----------------------------------------\n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    # Ignore the pad token in the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 4. Training Loop\n",
    "    # ----------------------------------------\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # FIX: The total number of batches is unknown, so we can't use len(train_loader).\n",
    "        # We will track the step count manually.\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Batches from DataLoader are already tensors\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # The model's output sequence length might be different from the input\n",
    "            # due to pooling. We need to truncate the labels to match.\n",
    "            outputs_seq_len = outputs.size(1)\n",
    "            labels = labels[:, :outputs_seq_len]\n",
    "\n",
    "            # Reshape for loss calculation: (B*L, V) vs (B*L)\n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            # FIX: Use .reshape() instead of .view() to handle non-contiguous tensors\n",
    "            labels_flat = labels.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            torch.save(model.state_dict(), 'language-model')\n",
    "        \n",
    "        # FIX: The total number of batches is unknown, so average loss is not meaningful.\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss:.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77b4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bert tokenizer...\n",
      "Initializing the model for demonstration purposes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4158/1642505007.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "An error occurred during text generation: Given input size: (256x1x1). Calculated output size: (256x1x0). Output size is too small\n",
      "\n",
      "--- Generated Text ---\n",
      "Text generation failed.\n",
      "----------------------\n",
      "\n",
      "Script execution finished.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, sequence_length=0):\n",
    "    \"\"\"\n",
    "    Generates text from the model given a starting prompt.\n",
    "    \"\"\"\n",
    "    print(\"Generating text...\")\n",
    "    \n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "        # Loop to generate new tokens\n",
    "        for _ in range(max_length):\n",
    "            # Pad the input_ids to the model's fixed sequence length\n",
    "            # This is necessary because the CNN architecture is not\n",
    "            # designed for variable-length inputs.\n",
    "            current_len = input_ids.size(1)\n",
    "            padding_needed = sequence_length - current_len\n",
    "            if padding_needed > 0:\n",
    "                padded_input_ids = F.pad(input_ids, (0, padding_needed), 'constant', tokenizer.pad_token_id)\n",
    "            else:\n",
    "                padded_input_ids = input_ids[:, -sequence_length:]\n",
    "\n",
    "            # Get the model's output on the padded sequence\n",
    "            outputs = model(padded_input_ids)\n",
    "\n",
    "            # Get the predictions for the last non-padded token\n",
    "            predictions = outputs[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, dim=-1)\n",
    "            \n",
    "            # Add the new token to the original sequence\n",
    "            input_ids = torch.cat([input_ids, predicted_id.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        # Decode the token IDs back to text.\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during text generation: {e}\")\n",
    "        return \"Text generation failed.\"\n",
    "\n",
    "sequence_length = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# 1. Load Bert tokenizer\n",
    "print(\"Loading Bert tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# 2. Initialize the model (without training)\n",
    "print(\"Initializing the model for demonstration purposes...\")\n",
    "model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "\n",
    "model_state_dict = torch.load('language-model', map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# 3. Define the starting prompt\n",
    "start_prompt = \"hello world\"\n",
    "\n",
    "# 4. Generate text with the (simulated) trained model\n",
    "generated_text = generate_text(model, tokenizer, start_prompt, sequence_length)\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
