{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd4a83fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55, 531, 25, 241, 12, 129, 394, 44, 492, 3326, 15068, 58, 148, 56, 43, 8, 1004, 6, 474, 48, 30, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 727, 1715, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 45, 301, 782, 3624, 14627, 15, 12612, 277, 5, 216, 56, 36, 2119, 3, 9, 19529, 593, 853, 21, 921, 113, 2746, 12, 129, 394, 28, 70, 17712, 1098, 5, 216, 56, 3884, 25, 762, 25, 174, 12, 214, 12, 5978, 16, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 379, 2097, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 11, 27856, 6, 303, 24190, 11, 1472, 251, 5, 37, 583, 12, 36, 16, 8, 853, 19, 25264, 399, 568, 6, 11, 21, 21380, 7, 34, 19, 339, 5, 15746, 26, 16, 8, 583, 56, 36, 893, 3, 9, 3, 17, 18, 9486, 42, 3, 9, 1409, 29, 11, 25, 56, 36, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the T5 tokenizer.\n",
    "# It's recommended to use a tokenizer from a pre-trained model like 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# 2. Load the C4 dataset.\n",
    "# The `streaming=True` argument is useful for huge datasets like C4 to avoid downloading the whole thing.\n",
    "c4_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "\n",
    "# 3. Define a tokenization function.\n",
    "# This function will be applied to each batch of data.\n",
    "def tokenize_function(examples):\n",
    "    # The T5 model expects a prefix for the task, for example \"denoise text: \".\n",
    "    # This is important for T5's pre-training objective.\n",
    "    # However, for a simple tokenization, we can just process the \"text\" field.\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# 4. Apply the tokenizer to the dataset using the map function.\n",
    "# `batched=True` processes the data in batches, which is much faster.\n",
    "tokenized_c4 = c4_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# You can now iterate through the tokenized dataset.\n",
    "for example in tokenized_c4:\n",
    "    print(example[\"input_ids\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d17fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 37\n",
      "Starting training...\n",
      "Epoch [1/20], Step [1/7], Loss: 3.7109\n",
      "Epoch [1/20], Step [2/7], Loss: 3.6706\n",
      "Epoch [1/20], Step [3/7], Loss: 3.6618\n",
      "Epoch [1/20], Step [4/7], Loss: 3.6554\n",
      "Epoch [1/20], Step [5/7], Loss: 3.6641\n",
      "Epoch [1/20], Step [6/7], Loss: 3.6367\n",
      "Epoch [1/20], Step [7/7], Loss: 3.6520\n",
      "Epoch [2/20], Step [1/7], Loss: 3.2856\n",
      "Epoch [2/20], Step [2/7], Loss: 3.2701\n",
      "Epoch [2/20], Step [3/7], Loss: 3.2480\n",
      "Epoch [2/20], Step [4/7], Loss: 3.3315\n",
      "Epoch [2/20], Step [5/7], Loss: 3.2014\n",
      "Epoch [2/20], Step [6/7], Loss: 3.2693\n",
      "Epoch [2/20], Step [7/7], Loss: 3.2356\n",
      "Epoch [3/20], Step [1/7], Loss: 3.5368\n",
      "Epoch [3/20], Step [2/7], Loss: 2.8795\n",
      "Epoch [3/20], Step [3/7], Loss: 2.9374\n",
      "Epoch [3/20], Step [4/7], Loss: 2.7491\n",
      "Epoch [3/20], Step [5/7], Loss: 2.8209\n",
      "Epoch [3/20], Step [6/7], Loss: 2.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130231/3157935005.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Step [7/7], Loss: 2.7118\n",
      "Epoch [4/20], Step [1/7], Loss: 2.7884\n",
      "Epoch [4/20], Step [2/7], Loss: 2.9727\n",
      "Epoch [4/20], Step [3/7], Loss: 2.7151\n",
      "Epoch [4/20], Step [4/7], Loss: 2.7521\n",
      "Epoch [4/20], Step [5/7], Loss: 2.9391\n",
      "Epoch [4/20], Step [6/7], Loss: 2.8336\n",
      "Epoch [4/20], Step [7/7], Loss: 2.9083\n",
      "Epoch [5/20], Step [1/7], Loss: 2.7260\n",
      "Epoch [5/20], Step [2/7], Loss: 2.7982\n",
      "Epoch [5/20], Step [3/7], Loss: 2.9113\n",
      "Epoch [5/20], Step [4/7], Loss: 2.3345\n",
      "Epoch [5/20], Step [5/7], Loss: 2.1588\n",
      "Epoch [5/20], Step [6/7], Loss: 2.1593\n",
      "Epoch [5/20], Step [7/7], Loss: 2.2315\n",
      "Epoch [6/20], Step [1/7], Loss: 1.4436\n",
      "Epoch [6/20], Step [2/7], Loss: 1.2955\n",
      "Epoch [6/20], Step [3/7], Loss: 1.6238\n",
      "Epoch [6/20], Step [4/7], Loss: 2.1200\n",
      "Epoch [6/20], Step [5/7], Loss: 1.8380\n",
      "Epoch [6/20], Step [6/7], Loss: 1.7514\n",
      "Epoch [6/20], Step [7/7], Loss: 1.6431\n",
      "Epoch [7/20], Step [1/7], Loss: 1.3238\n",
      "Epoch [7/20], Step [2/7], Loss: 1.2145\n",
      "Epoch [7/20], Step [3/7], Loss: 1.7978\n",
      "Epoch [7/20], Step [4/7], Loss: 1.6657\n",
      "Epoch [7/20], Step [5/7], Loss: 1.6501\n",
      "Epoch [7/20], Step [6/7], Loss: 1.6469\n",
      "Epoch [7/20], Step [7/7], Loss: 1.4383\n",
      "Epoch [8/20], Step [1/7], Loss: 1.5700\n",
      "Epoch [8/20], Step [2/7], Loss: 1.2023\n",
      "Epoch [8/20], Step [3/7], Loss: 1.3192\n",
      "Epoch [8/20], Step [4/7], Loss: 1.1158\n",
      "Epoch [8/20], Step [5/7], Loss: 1.0496\n",
      "Epoch [8/20], Step [6/7], Loss: 1.0011\n",
      "Epoch [8/20], Step [7/7], Loss: 0.9057\n",
      "Epoch [9/20], Step [1/7], Loss: 1.0839\n",
      "Epoch [9/20], Step [2/7], Loss: 0.6836\n",
      "Epoch [9/20], Step [3/7], Loss: 0.6346\n",
      "Epoch [9/20], Step [4/7], Loss: 0.4844\n",
      "Epoch [9/20], Step [5/7], Loss: 0.7037\n",
      "Epoch [9/20], Step [6/7], Loss: 0.8145\n",
      "Epoch [9/20], Step [7/7], Loss: 0.7023\n",
      "Epoch [10/20], Step [1/7], Loss: 0.0735\n",
      "Epoch [10/20], Step [2/7], Loss: 0.0437\n",
      "Epoch [10/20], Step [3/7], Loss: 0.1617\n",
      "Epoch [10/20], Step [4/7], Loss: 0.2437\n",
      "Epoch [10/20], Step [5/7], Loss: 0.3822\n",
      "Epoch [10/20], Step [6/7], Loss: 0.4081\n",
      "Epoch [10/20], Step [7/7], Loss: 0.5241\n",
      "Epoch [11/20], Step [1/7], Loss: 0.0741\n",
      "Epoch [11/20], Step [2/7], Loss: 0.3561\n",
      "Epoch [11/20], Step [3/7], Loss: 0.2576\n",
      "Epoch [11/20], Step [4/7], Loss: 0.2721\n",
      "Epoch [11/20], Step [5/7], Loss: 0.2875\n",
      "Epoch [11/20], Step [6/7], Loss: 0.2955\n",
      "Epoch [11/20], Step [7/7], Loss: 0.2535\n",
      "Epoch [12/20], Step [1/7], Loss: 0.0298\n",
      "Epoch [12/20], Step [2/7], Loss: 0.1796\n",
      "Epoch [12/20], Step [3/7], Loss: 0.4457\n",
      "Epoch [12/20], Step [4/7], Loss: 0.7244\n",
      "Epoch [12/20], Step [5/7], Loss: 0.5890\n",
      "Epoch [12/20], Step [6/7], Loss: 0.6573\n",
      "Epoch [12/20], Step [7/7], Loss: 0.5650\n",
      "Epoch [13/20], Step [1/7], Loss: 0.0017\n",
      "Epoch [13/20], Step [2/7], Loss: 0.1937\n",
      "Epoch [13/20], Step [3/7], Loss: 0.1298\n",
      "Epoch [13/20], Step [4/7], Loss: 0.4717\n",
      "Epoch [13/20], Step [5/7], Loss: 0.3878\n",
      "Epoch [13/20], Step [6/7], Loss: 0.3279\n",
      "Epoch [13/20], Step [7/7], Loss: 0.2928\n",
      "Epoch [14/20], Step [1/7], Loss: 1.1123\n",
      "Epoch [14/20], Step [2/7], Loss: 0.5727\n",
      "Epoch [14/20], Step [3/7], Loss: 0.3840\n",
      "Epoch [14/20], Step [4/7], Loss: 0.2972\n",
      "Epoch [14/20], Step [5/7], Loss: 0.2789\n",
      "Epoch [14/20], Step [6/7], Loss: 0.2336\n",
      "Epoch [14/20], Step [7/7], Loss: 0.2017\n",
      "Epoch [15/20], Step [1/7], Loss: 0.0346\n",
      "Epoch [15/20], Step [2/7], Loss: 0.0347\n",
      "Epoch [15/20], Step [3/7], Loss: 0.0349\n",
      "Epoch [15/20], Step [4/7], Loss: 0.0299\n",
      "Epoch [15/20], Step [5/7], Loss: 0.0250\n",
      "Epoch [15/20], Step [6/7], Loss: 0.0220\n",
      "Epoch [15/20], Step [7/7], Loss: 0.0306\n",
      "Epoch [16/20], Step [1/7], Loss: 0.5794\n",
      "Epoch [16/20], Step [2/7], Loss: 0.3182\n",
      "Epoch [16/20], Step [3/7], Loss: 0.2339\n",
      "Epoch [16/20], Step [4/7], Loss: 0.1766\n",
      "Epoch [16/20], Step [5/7], Loss: 0.1703\n",
      "Epoch [16/20], Step [6/7], Loss: 0.1519\n",
      "Epoch [16/20], Step [7/7], Loss: 0.1306\n",
      "Epoch [17/20], Step [1/7], Loss: 0.0012\n",
      "Epoch [17/20], Step [2/7], Loss: 0.0360\n",
      "Epoch [17/20], Step [3/7], Loss: 0.1609\n",
      "Epoch [17/20], Step [4/7], Loss: 0.1262\n",
      "Epoch [17/20], Step [5/7], Loss: 0.1017\n",
      "Epoch [17/20], Step [6/7], Loss: 0.1043\n",
      "Epoch [17/20], Step [7/7], Loss: 0.1367\n",
      "Epoch [18/20], Step [1/7], Loss: 0.0038\n",
      "Epoch [18/20], Step [2/7], Loss: 0.0753\n",
      "Epoch [18/20], Step [3/7], Loss: 0.0502\n",
      "Epoch [18/20], Step [4/7], Loss: 0.0387\n",
      "Epoch [18/20], Step [5/7], Loss: 0.5492\n",
      "Epoch [18/20], Step [6/7], Loss: 0.4722\n",
      "Epoch [18/20], Step [7/7], Loss: 0.4047\n",
      "Epoch [19/20], Step [1/7], Loss: 0.0002\n",
      "Epoch [19/20], Step [2/7], Loss: 0.0002\n",
      "Epoch [19/20], Step [3/7], Loss: 0.0027\n",
      "Epoch [19/20], Step [4/7], Loss: 0.0021\n",
      "Epoch [19/20], Step [5/7], Loss: 0.0018\n",
      "Epoch [19/20], Step [6/7], Loss: 0.0018\n",
      "Epoch [19/20], Step [7/7], Loss: 0.0028\n",
      "Epoch [20/20], Step [1/7], Loss: 0.0150\n",
      "Epoch [20/20], Step [2/7], Loss: 0.0189\n",
      "Epoch [20/20], Step [3/7], Loss: 0.0212\n",
      "Epoch [20/20], Step [4/7], Loss: 0.1302\n",
      "Epoch [20/20], Step [5/7], Loss: 0.1549\n",
      "Epoch [20/20], Step [6/7], Loss: 0.1490\n",
      "Epoch [20/20], Step [7/7], Loss: 0.2719\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Define a simple text dataset and vocabulary ---\n",
    "# FIX: Added much longer sentences to the training data.\n",
    "TEXT_DATA = [\n",
    "    \"hello world and this is a much longer sentence than before\",\n",
    "    \"python programming is fun and easy to learn especially for beginners\",\n",
    "    \"i love deep learning with convolutional neural networks and attention\",\n",
    "    \"deep learning is a powerful tool in artificial intelligence for language modeling tasks\"\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = sorted(list(set(\" \".join(TEXT_DATA).split())))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, word_to_idx, sequence_length):\n",
    "        self.sequences = []\n",
    "        for sentence in text_data:\n",
    "            words = sentence.split()\n",
    "            # This condition will now be met for the longer sentences\n",
    "            if len(words) > sequence_length:\n",
    "                for i in range(len(words) - sequence_length):\n",
    "                    input_seq = [word_to_idx[word] for word in words[i:i+sequence_length]]\n",
    "                    target_word = word_to_idx[words[i+sequence_length]]\n",
    "                    self.sequences.append((torch.tensor(input_seq), torch.tensor(target_word)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# --- 2. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 3. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * self.final_seq_len, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 2\n",
    "    num_epochs = 20\n",
    "    # FIX: Increased sequence length to a value that won't result in an empty tensor.\n",
    "    sequence_length = 8\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Data loading\n",
    "    train_dataset = TextDataset(TEXT_DATA, word_to_idx, sequence_length)\n",
    "    \n",
    "    # FIX: Added a check to prevent the error\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "        print(f\"Please increase the length of sentences in TEXT_DATA or decrease the `sequence_length` (currently {sequence_length}).\")\n",
    "        return\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e318639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading a small streaming portion of the C4 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162852/1185293299.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/25], Step [50], Loss: 8.1715\n",
      "Epoch [1/25], Step [100], Loss: 7.9306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 194\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    179\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    183\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        # We need a causality mask for language modeling\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 2. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # Conv1d expects input shape (batch, channels, sequence_length)\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        # FIX: Removed nn.Flatten() and adjusted Linear layer to work on a per-token basis.\n",
    "        # This is the correct way to build a sequence-to-sequence model for language modeling.\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        # FIX: The fc_block now processes the attended features without flattening.\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "# --- 3. Main training function updated for C4 dataset ---\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 8\n",
    "    num_epochs = 25\n",
    "    sequence_length = 128\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 1. Load C4 dataset (streaming) and Bert tokenizer\n",
    "    # ----------------------------------------\n",
    "    print(\"Loading a small streaming portion of the C4 dataset...\")\n",
    "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Use a simpler, pre-trained tokenizer: BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    # We will use a smaller sample for a quicker demonstration.\n",
    "    dataset_sample = dataset\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. Pre-process the dataset with the tokenizer\n",
    "    # ----------------------------------------\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"text\"]\n",
    "        tokenized_input = tokenizer(\n",
    "            inputs,\n",
    "            max_length=sequence_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Shift the labels for next-token prediction\n",
    "        input_ids = tokenized_input['input_ids'].squeeze(0)\n",
    "        labels = torch.cat((input_ids[1:], torch.tensor([tokenizer.pad_token_id])))\n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "    \n",
    "    # Apply the preprocessing to the streaming dataset\n",
    "    processed_dataset = dataset_sample.map(preprocess_function, batched=False)\n",
    "    \n",
    "    # We use the processed streaming dataset directly with the DataLoader\n",
    "    train_loader = DataLoader(processed_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 3. Model, Loss, and Optimizer\n",
    "    # ----------------------------------------\n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    # Ignore the pad token in the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 4. Training Loop\n",
    "    # ----------------------------------------\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # FIX: The total number of batches is unknown, so we can't use len(train_loader).\n",
    "        # We will track the step count manually.\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Batches from DataLoader are already tensors\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # The model's output sequence length might be different from the input\n",
    "            # due to pooling. We need to truncate the labels to match.\n",
    "            outputs_seq_len = outputs.size(1)\n",
    "            labels = labels[:, :outputs_seq_len]\n",
    "\n",
    "            # Reshape for loss calculation: (B*L, V) vs (B*L)\n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            # FIX: Use .reshape() instead of .view() to handle non-contiguous tensors\n",
    "            labels_flat = labels.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # FIX: The total number of batches is unknown, so average loss is not meaningful.\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss:.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2143dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bert tokenizer...\n",
      "Initializing the model for demonstration purposes...\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162852/1185293299.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "hello [unused947] composuredad ranged pleaded amplifiers napier shanghai halves napier amplifiers modify napier specimens unity [unused947] securities canberrawarkax correspondence bucharest ableguide mccormickquet fantasiaᆨ blonde aromatic composure bob weak lifetime remembrance [unused947] faculties bucharest abledad modify canberraoge subsidy mead architecture modify strange [unused701] able\n",
      "----------------------\n",
      "\n",
      "Script execution finished.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text from the model given a starting prompt.\n",
    "    \"\"\"\n",
    "    print(\"Generating text...\")\n",
    "    \n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "        # Loop to generate new tokens\n",
    "        for _ in range(max_length):\n",
    "            # Pad the input_ids to the model's fixed sequence length\n",
    "            # This is necessary because the CNN architecture is not\n",
    "            # designed for variable-length inputs.\n",
    "            current_len = input_ids.size(1)\n",
    "            padding_needed = model.sequence_length - current_len\n",
    "            if padding_needed > 0:\n",
    "                padded_input_ids = F.pad(input_ids, (0, padding_needed), 'constant', tokenizer.pad_token_id)\n",
    "            else:\n",
    "                padded_input_ids = input_ids[:, -model.sequence_length:]\n",
    "\n",
    "            # Get the model's output on the padded sequence\n",
    "            outputs = model(padded_input_ids)\n",
    "\n",
    "            # Get the predictions for the last non-padded token\n",
    "            predictions = outputs[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, dim=-1)\n",
    "            \n",
    "            # Add the new token to the original sequence\n",
    "            input_ids = torch.cat([input_ids, predicted_id.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        # Decode the token IDs back to text.\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during text generation: {e}\")\n",
    "        return \"Text generation failed.\"\n",
    "\n",
    "sequence_length = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# 1. Load Bert tokenizer\n",
    "print(\"Loading Bert tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# 2. Initialize the model (without training)\n",
    "print(\"Initializing the model for demonstration purposes...\")\n",
    "model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "\n",
    "# 3. Define the starting prompt\n",
    "start_prompt = \"hello\"\n",
    "\n",
    "# 4. Generate text with the (simulated) trained model\n",
    "generated_text = generate_text(model, tokenizer, start_prompt)\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
