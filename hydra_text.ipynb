{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd4a83fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55, 531, 25, 241, 12, 129, 394, 44, 492, 3326, 15068, 58, 148, 56, 43, 8, 1004, 6, 474, 48, 30, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 727, 1715, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 45, 301, 782, 3624, 14627, 15, 12612, 277, 5, 216, 56, 36, 2119, 3, 9, 19529, 593, 853, 21, 921, 113, 2746, 12, 129, 394, 28, 70, 17712, 1098, 5, 216, 56, 3884, 25, 762, 25, 174, 12, 214, 12, 5978, 16, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 379, 2097, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 11, 27856, 6, 303, 24190, 11, 1472, 251, 5, 37, 583, 12, 36, 16, 8, 853, 19, 25264, 399, 568, 6, 11, 21, 21380, 7, 34, 19, 339, 5, 15746, 26, 16, 8, 583, 56, 36, 893, 3, 9, 3, 17, 18, 9486, 42, 3, 9, 1409, 29, 11, 25, 56, 36, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the T5 tokenizer.\n",
    "# It's recommended to use a tokenizer from a pre-trained model like 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# 2. Load the C4 dataset.\n",
    "# The `streaming=True` argument is useful for huge datasets like C4 to avoid downloading the whole thing.\n",
    "c4_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "\n",
    "# 3. Define a tokenization function.\n",
    "# This function will be applied to each batch of data.\n",
    "def tokenize_function(examples):\n",
    "    # The T5 model expects a prefix for the task, for example \"denoise text: \".\n",
    "    # This is important for T5's pre-training objective.\n",
    "    # However, for a simple tokenization, we can just process the \"text\" field.\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# 4. Apply the tokenizer to the dataset using the map function.\n",
    "# `batched=True` processes the data in batches, which is much faster.\n",
    "tokenized_c4 = c4_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# You can now iterate through the tokenized dataset.\n",
    "for example in tokenized_c4:\n",
    "    print(example[\"input_ids\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d17fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 37\n",
      "Starting training...\n",
      "Epoch [1/20], Step [1/7], Loss: 3.7109\n",
      "Epoch [1/20], Step [2/7], Loss: 3.6706\n",
      "Epoch [1/20], Step [3/7], Loss: 3.6618\n",
      "Epoch [1/20], Step [4/7], Loss: 3.6554\n",
      "Epoch [1/20], Step [5/7], Loss: 3.6641\n",
      "Epoch [1/20], Step [6/7], Loss: 3.6367\n",
      "Epoch [1/20], Step [7/7], Loss: 3.6520\n",
      "Epoch [2/20], Step [1/7], Loss: 3.2856\n",
      "Epoch [2/20], Step [2/7], Loss: 3.2701\n",
      "Epoch [2/20], Step [3/7], Loss: 3.2480\n",
      "Epoch [2/20], Step [4/7], Loss: 3.3315\n",
      "Epoch [2/20], Step [5/7], Loss: 3.2014\n",
      "Epoch [2/20], Step [6/7], Loss: 3.2693\n",
      "Epoch [2/20], Step [7/7], Loss: 3.2356\n",
      "Epoch [3/20], Step [1/7], Loss: 3.5368\n",
      "Epoch [3/20], Step [2/7], Loss: 2.8795\n",
      "Epoch [3/20], Step [3/7], Loss: 2.9374\n",
      "Epoch [3/20], Step [4/7], Loss: 2.7491\n",
      "Epoch [3/20], Step [5/7], Loss: 2.8209\n",
      "Epoch [3/20], Step [6/7], Loss: 2.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130231/3157935005.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Step [7/7], Loss: 2.7118\n",
      "Epoch [4/20], Step [1/7], Loss: 2.7884\n",
      "Epoch [4/20], Step [2/7], Loss: 2.9727\n",
      "Epoch [4/20], Step [3/7], Loss: 2.7151\n",
      "Epoch [4/20], Step [4/7], Loss: 2.7521\n",
      "Epoch [4/20], Step [5/7], Loss: 2.9391\n",
      "Epoch [4/20], Step [6/7], Loss: 2.8336\n",
      "Epoch [4/20], Step [7/7], Loss: 2.9083\n",
      "Epoch [5/20], Step [1/7], Loss: 2.7260\n",
      "Epoch [5/20], Step [2/7], Loss: 2.7982\n",
      "Epoch [5/20], Step [3/7], Loss: 2.9113\n",
      "Epoch [5/20], Step [4/7], Loss: 2.3345\n",
      "Epoch [5/20], Step [5/7], Loss: 2.1588\n",
      "Epoch [5/20], Step [6/7], Loss: 2.1593\n",
      "Epoch [5/20], Step [7/7], Loss: 2.2315\n",
      "Epoch [6/20], Step [1/7], Loss: 1.4436\n",
      "Epoch [6/20], Step [2/7], Loss: 1.2955\n",
      "Epoch [6/20], Step [3/7], Loss: 1.6238\n",
      "Epoch [6/20], Step [4/7], Loss: 2.1200\n",
      "Epoch [6/20], Step [5/7], Loss: 1.8380\n",
      "Epoch [6/20], Step [6/7], Loss: 1.7514\n",
      "Epoch [6/20], Step [7/7], Loss: 1.6431\n",
      "Epoch [7/20], Step [1/7], Loss: 1.3238\n",
      "Epoch [7/20], Step [2/7], Loss: 1.2145\n",
      "Epoch [7/20], Step [3/7], Loss: 1.7978\n",
      "Epoch [7/20], Step [4/7], Loss: 1.6657\n",
      "Epoch [7/20], Step [5/7], Loss: 1.6501\n",
      "Epoch [7/20], Step [6/7], Loss: 1.6469\n",
      "Epoch [7/20], Step [7/7], Loss: 1.4383\n",
      "Epoch [8/20], Step [1/7], Loss: 1.5700\n",
      "Epoch [8/20], Step [2/7], Loss: 1.2023\n",
      "Epoch [8/20], Step [3/7], Loss: 1.3192\n",
      "Epoch [8/20], Step [4/7], Loss: 1.1158\n",
      "Epoch [8/20], Step [5/7], Loss: 1.0496\n",
      "Epoch [8/20], Step [6/7], Loss: 1.0011\n",
      "Epoch [8/20], Step [7/7], Loss: 0.9057\n",
      "Epoch [9/20], Step [1/7], Loss: 1.0839\n",
      "Epoch [9/20], Step [2/7], Loss: 0.6836\n",
      "Epoch [9/20], Step [3/7], Loss: 0.6346\n",
      "Epoch [9/20], Step [4/7], Loss: 0.4844\n",
      "Epoch [9/20], Step [5/7], Loss: 0.7037\n",
      "Epoch [9/20], Step [6/7], Loss: 0.8145\n",
      "Epoch [9/20], Step [7/7], Loss: 0.7023\n",
      "Epoch [10/20], Step [1/7], Loss: 0.0735\n",
      "Epoch [10/20], Step [2/7], Loss: 0.0437\n",
      "Epoch [10/20], Step [3/7], Loss: 0.1617\n",
      "Epoch [10/20], Step [4/7], Loss: 0.2437\n",
      "Epoch [10/20], Step [5/7], Loss: 0.3822\n",
      "Epoch [10/20], Step [6/7], Loss: 0.4081\n",
      "Epoch [10/20], Step [7/7], Loss: 0.5241\n",
      "Epoch [11/20], Step [1/7], Loss: 0.0741\n",
      "Epoch [11/20], Step [2/7], Loss: 0.3561\n",
      "Epoch [11/20], Step [3/7], Loss: 0.2576\n",
      "Epoch [11/20], Step [4/7], Loss: 0.2721\n",
      "Epoch [11/20], Step [5/7], Loss: 0.2875\n",
      "Epoch [11/20], Step [6/7], Loss: 0.2955\n",
      "Epoch [11/20], Step [7/7], Loss: 0.2535\n",
      "Epoch [12/20], Step [1/7], Loss: 0.0298\n",
      "Epoch [12/20], Step [2/7], Loss: 0.1796\n",
      "Epoch [12/20], Step [3/7], Loss: 0.4457\n",
      "Epoch [12/20], Step [4/7], Loss: 0.7244\n",
      "Epoch [12/20], Step [5/7], Loss: 0.5890\n",
      "Epoch [12/20], Step [6/7], Loss: 0.6573\n",
      "Epoch [12/20], Step [7/7], Loss: 0.5650\n",
      "Epoch [13/20], Step [1/7], Loss: 0.0017\n",
      "Epoch [13/20], Step [2/7], Loss: 0.1937\n",
      "Epoch [13/20], Step [3/7], Loss: 0.1298\n",
      "Epoch [13/20], Step [4/7], Loss: 0.4717\n",
      "Epoch [13/20], Step [5/7], Loss: 0.3878\n",
      "Epoch [13/20], Step [6/7], Loss: 0.3279\n",
      "Epoch [13/20], Step [7/7], Loss: 0.2928\n",
      "Epoch [14/20], Step [1/7], Loss: 1.1123\n",
      "Epoch [14/20], Step [2/7], Loss: 0.5727\n",
      "Epoch [14/20], Step [3/7], Loss: 0.3840\n",
      "Epoch [14/20], Step [4/7], Loss: 0.2972\n",
      "Epoch [14/20], Step [5/7], Loss: 0.2789\n",
      "Epoch [14/20], Step [6/7], Loss: 0.2336\n",
      "Epoch [14/20], Step [7/7], Loss: 0.2017\n",
      "Epoch [15/20], Step [1/7], Loss: 0.0346\n",
      "Epoch [15/20], Step [2/7], Loss: 0.0347\n",
      "Epoch [15/20], Step [3/7], Loss: 0.0349\n",
      "Epoch [15/20], Step [4/7], Loss: 0.0299\n",
      "Epoch [15/20], Step [5/7], Loss: 0.0250\n",
      "Epoch [15/20], Step [6/7], Loss: 0.0220\n",
      "Epoch [15/20], Step [7/7], Loss: 0.0306\n",
      "Epoch [16/20], Step [1/7], Loss: 0.5794\n",
      "Epoch [16/20], Step [2/7], Loss: 0.3182\n",
      "Epoch [16/20], Step [3/7], Loss: 0.2339\n",
      "Epoch [16/20], Step [4/7], Loss: 0.1766\n",
      "Epoch [16/20], Step [5/7], Loss: 0.1703\n",
      "Epoch [16/20], Step [6/7], Loss: 0.1519\n",
      "Epoch [16/20], Step [7/7], Loss: 0.1306\n",
      "Epoch [17/20], Step [1/7], Loss: 0.0012\n",
      "Epoch [17/20], Step [2/7], Loss: 0.0360\n",
      "Epoch [17/20], Step [3/7], Loss: 0.1609\n",
      "Epoch [17/20], Step [4/7], Loss: 0.1262\n",
      "Epoch [17/20], Step [5/7], Loss: 0.1017\n",
      "Epoch [17/20], Step [6/7], Loss: 0.1043\n",
      "Epoch [17/20], Step [7/7], Loss: 0.1367\n",
      "Epoch [18/20], Step [1/7], Loss: 0.0038\n",
      "Epoch [18/20], Step [2/7], Loss: 0.0753\n",
      "Epoch [18/20], Step [3/7], Loss: 0.0502\n",
      "Epoch [18/20], Step [4/7], Loss: 0.0387\n",
      "Epoch [18/20], Step [5/7], Loss: 0.5492\n",
      "Epoch [18/20], Step [6/7], Loss: 0.4722\n",
      "Epoch [18/20], Step [7/7], Loss: 0.4047\n",
      "Epoch [19/20], Step [1/7], Loss: 0.0002\n",
      "Epoch [19/20], Step [2/7], Loss: 0.0002\n",
      "Epoch [19/20], Step [3/7], Loss: 0.0027\n",
      "Epoch [19/20], Step [4/7], Loss: 0.0021\n",
      "Epoch [19/20], Step [5/7], Loss: 0.0018\n",
      "Epoch [19/20], Step [6/7], Loss: 0.0018\n",
      "Epoch [19/20], Step [7/7], Loss: 0.0028\n",
      "Epoch [20/20], Step [1/7], Loss: 0.0150\n",
      "Epoch [20/20], Step [2/7], Loss: 0.0189\n",
      "Epoch [20/20], Step [3/7], Loss: 0.0212\n",
      "Epoch [20/20], Step [4/7], Loss: 0.1302\n",
      "Epoch [20/20], Step [5/7], Loss: 0.1549\n",
      "Epoch [20/20], Step [6/7], Loss: 0.1490\n",
      "Epoch [20/20], Step [7/7], Loss: 0.2719\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Define a simple text dataset and vocabulary ---\n",
    "# FIX: Added much longer sentences to the training data.\n",
    "TEXT_DATA = [\n",
    "    \"hello world and this is a much longer sentence than before\",\n",
    "    \"python programming is fun and easy to learn especially for beginners\",\n",
    "    \"i love deep learning with convolutional neural networks and attention\",\n",
    "    \"deep learning is a powerful tool in artificial intelligence for language modeling tasks\"\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = sorted(list(set(\" \".join(TEXT_DATA).split())))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, word_to_idx, sequence_length):\n",
    "        self.sequences = []\n",
    "        for sentence in text_data:\n",
    "            words = sentence.split()\n",
    "            # This condition will now be met for the longer sentences\n",
    "            if len(words) > sequence_length:\n",
    "                for i in range(len(words) - sequence_length):\n",
    "                    input_seq = [word_to_idx[word] for word in words[i:i+sequence_length]]\n",
    "                    target_word = word_to_idx[words[i+sequence_length]]\n",
    "                    self.sequences.append((torch.tensor(input_seq), torch.tensor(target_word)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# --- 2. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 3. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * self.final_seq_len, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 2\n",
    "    num_epochs = 20\n",
    "    # FIX: Increased sequence length to a value that won't result in an empty tensor.\n",
    "    sequence_length = 8\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Data loading\n",
    "    train_dataset = TextDataset(TEXT_DATA, word_to_idx, sequence_length)\n",
    "    \n",
    "    # FIX: Added a check to prevent the error\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "        print(f\"Please increase the length of sentences in TEXT_DATA or decrease the `sequence_length` (currently {sequence_length}).\")\n",
    "        return\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e318639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading a small streaming portion of the C4 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142525/1185293299.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/25], Step [50], Loss: 8.2171\n",
      "Epoch [1/25], Step [100], Loss: 7.9007\n",
      "Epoch [1/25], Step [150], Loss: 7.9703\n",
      "Epoch [1/25], Step [200], Loss: 7.5912\n",
      "Epoch [1/25], Step [250], Loss: 7.4627\n",
      "Epoch [1/25], Step [300], Loss: 7.5138\n",
      "Epoch [1/25], Step [350], Loss: 7.6503\n",
      "Epoch [1/25], Step [400], Loss: 7.6589\n",
      "Epoch [1/25], Step [450], Loss: 7.5327\n",
      "Epoch [1/25], Step [500], Loss: 7.4105\n",
      "Epoch [1/25], Step [550], Loss: 7.8069\n",
      "Epoch [1/25], Step [600], Loss: 7.5903\n",
      "Epoch [1/25], Step [650], Loss: 7.5068\n",
      "Epoch [1/25], Step [700], Loss: 7.4705\n",
      "Epoch [1/25], Step [750], Loss: 7.4707\n",
      "Epoch [1/25], Step [800], Loss: 7.9578\n",
      "Epoch [1/25], Step [850], Loss: 7.6668\n",
      "Epoch [1/25], Step [900], Loss: 7.6860\n",
      "Epoch [1/25], Step [950], Loss: 7.5823\n",
      "Epoch [1/25], Step [1000], Loss: 7.6852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 194\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 158\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# FIX: The total number of batches is unknown, so we can't use len(train_loader).\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# We will track the step count manually.\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Batches from DataLoader are already tensors\u001b[39;49;00m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter))\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:2361\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an Iterable Dataset from a generator.\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m \n\u001b[1;32m   2318\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[0;32m-> 2361\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator, features\u001b[38;5;241m=\u001b[39mfeatures, gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, split\u001b[38;5;241m=\u001b[39msplit\n\u001b[1;32m   2362\u001b[0m )\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:1107\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1104\u001b[0m def iter_batched_inputs():\n\u001b[1;32m   1105\u001b[0m     nonlocal current_idx\n\u001b[1;32m   1106\u001b[0m     for key, example in iterator:\n\u001b[0;32m-> 1107\u001b[0m         # If `batched`, first build the batch, if `batch_size` is None or <=0, then the batch is the whole dataset\n\u001b[1;32m   1108\u001b[0m         iterator_batch = (\n\u001b[1;32m   1109\u001b[0m             iterator\n\u001b[1;32m   1110\u001b[0m             if self.batch_size is None or self.batch_size <= 0\n\u001b[1;32m   1111\u001b[0m             else islice(iterator, self.batch_size - 1)\n\u001b[1;32m   1112\u001b[0m         )\n\u001b[1;32m   1113\u001b[0m         key_examples_list = [(key, example)] + list(iterator_batch)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:1286\u001b[0m, in \u001b[0;36m_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39miter_arrow()\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1286\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m _convert_to_arrow(\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable,\n\u001b[1;32m   1288\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1289\u001b[0m         drop_last_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last_batch,\n\u001b[1;32m   1290\u001b[0m     )\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:1271\u001b[0m, in \u001b[0;36miter_outputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, transformed_example\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loop:\n\u001b[1;32m   1272\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanceling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m async tasks.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:1201\u001b[0m, in \u001b[0;36mapply_function\u001b[0;34m(key_example, indices)\u001b[0m\n\u001b[1;32m   1199\u001b[0m inputs_iterator \u001b[38;5;241m=\u001b[39m iter_batched_inputs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched \u001b[38;5;28;01melse\u001b[39;00m iter_inputs()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction):\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict:\n\u001b[1;32m   1202\u001b[0m         previous_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m previous_state\n",
      "Cell \u001b[0;32mIn[14], line 120\u001b[0m, in \u001b[0;36mtrain_model.<locals>.preprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m    119\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 120\u001b[0m     tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Shift the labels for next-token prediction\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenized_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2887\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2887\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2997\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2976\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2977\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2994\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2995\u001b[0m     )\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3016\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3064\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3065\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3066\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3070\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3071\u001b[0m )\n\u001b[0;32m-> 3073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:800\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m     )\n\u001b[0;32m--> 800\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    804\u001b[0m     first_ids,\n\u001b[1;32m    805\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    821\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:767\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 767\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:661\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()  \u001b[38;5;66;03m# don't split on any of the added tokens\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# \"This is something<special_token_1>  else\"\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:-1\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Attention Mechanism for 1D sequences ---\n",
    "class AttentionMessagePassing(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(AttentionMessagePassing, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.query_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.key_proj = nn.Linear(in_features, in_features // 2)\n",
    "        self.value_proj = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        key_t = key.permute(0, 2, 1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query, key_t)\n",
    "        attention_scores = attention_scores / np.sqrt(query.size(-1))\n",
    "        \n",
    "        # We need a causality mask for language modeling\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        attention_scores.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        message_passed_features = torch.bmm(attention_weights, value)\n",
    "        \n",
    "        output = message_passed_features + x\n",
    "        return output\n",
    "\n",
    "# --- 2. The new TextCNN Architecture with Attention ---\n",
    "class AttentionTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
    "        super(AttentionTextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # Conv1d expects input shape (batch, channels, sequence_length)\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.final_seq_len = self._compute_final_seq_len()\n",
    "        \n",
    "        self.attention_block = AttentionMessagePassing(in_features=256)\n",
    "        \n",
    "        # FIX: Removed nn.Flatten() and adjusted Linear layer to work on a per-token basis.\n",
    "        # This is the correct way to build a sequence-to-sequence model for language modeling.\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def _compute_final_seq_len(self):\n",
    "        l_in = self.sequence_length\n",
    "        l_out_conv1 = (l_in + 2*1 - 3) + 1\n",
    "        l_out_pool1 = torch.floor(torch.tensor((l_out_conv1 - 2) / 2)) + 1\n",
    "        l_out_conv2 = (l_out_pool1 + 2*1 - 3) + 1\n",
    "        l_out_pool2 = torch.floor(torch.tensor((l_out_conv2 - 2) / 2)) + 1\n",
    "        \n",
    "        return int(l_out_pool2.item())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_features = self.conv_block(embeddings)\n",
    "        conv_features_t = conv_features.permute(0, 2, 1)\n",
    "        attended_features = self.attention_block(conv_features_t)\n",
    "        # FIX: The fc_block now processes the attended features without flattening.\n",
    "        logits = self.fc_block(attended_features)\n",
    "        return logits\n",
    "\n",
    "# --- 3. Main training function updated for C4 dataset ---\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 8\n",
    "    num_epochs = 25\n",
    "    sequence_length = 128\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 1. Load C4 dataset (streaming) and Bert tokenizer\n",
    "    # ----------------------------------------\n",
    "    print(\"Loading a small streaming portion of the C4 dataset...\")\n",
    "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Use a simpler, pre-trained tokenizer: BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    # We will use a smaller sample for a quicker demonstration.\n",
    "    dataset_sample = dataset\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. Pre-process the dataset with the tokenizer\n",
    "    # ----------------------------------------\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"text\"]\n",
    "        tokenized_input = tokenizer(\n",
    "            inputs,\n",
    "            max_length=sequence_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Shift the labels for next-token prediction\n",
    "        input_ids = tokenized_input['input_ids'].squeeze(0)\n",
    "        labels = torch.cat((input_ids[1:], torch.tensor([tokenizer.pad_token_id])))\n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "    \n",
    "    # Apply the preprocessing to the streaming dataset\n",
    "    processed_dataset = dataset_sample.map(preprocess_function, batched=False)\n",
    "    \n",
    "    # We use the processed streaming dataset directly with the DataLoader\n",
    "    train_loader = DataLoader(processed_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 3. Model, Loss, and Optimizer\n",
    "    # ----------------------------------------\n",
    "    model = AttentionTextCNN(vocab_size, embedding_dim, sequence_length).to(device)\n",
    "    # Ignore the pad token in the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 4. Training Loop\n",
    "    # ----------------------------------------\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # FIX: The total number of batches is unknown, so we can't use len(train_loader).\n",
    "        # We will track the step count manually.\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Batches from DataLoader are already tensors\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # The model's output sequence length might be different from the input\n",
    "            # due to pooling. We need to truncate the labels to match.\n",
    "            outputs_seq_len = outputs.size(1)\n",
    "            labels = labels[:, :outputs_seq_len]\n",
    "\n",
    "            # Reshape for loss calculation: (B*L, V) vs (B*L)\n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            # FIX: Use .reshape() instead of .view() to handle non-contiguous tensors\n",
    "            labels_flat = labels.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # FIX: The total number of batches is unknown, so average loss is not meaningful.\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss:.4f}')\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
